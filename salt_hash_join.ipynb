{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SHUFFLE_PARTITIONS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/19 16:20:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"salaries\")\n",
    "         .config(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", SHUFFLE_PARTITIONS)\n",
    "         .getOrCreate()\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading employee and salary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employees = (spark\n",
    "                .read\n",
    "                .option(\"header\", True)\n",
    "                .option(\"delimiter\", \";\")\n",
    "                .option(\"inferSchema\", True)\n",
    "                .csv(\"inputs/employees_10000.csv\")\n",
    "                )\n",
    "\n",
    "df_salaries = (spark\n",
    "               .read\n",
    "               .option(\"header\", True)\n",
    "               .option(\"delimiter\", \";\")\n",
    "               .option(\"inferSchema\", True)\n",
    "               .csv(\"inputs/salaries_10000.csv\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying imbalanced data distribution across partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining and displaying employee and salary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------------------+-----------+--------+\n",
      "|employee_id|salary_id|                name| department|  salary|\n",
      "+-----------+---------+--------------------+-----------+--------+\n",
      "|          1|        1|    Frederico Santos|  Mercearia|23708.05|\n",
      "|          2|        2|Alessandra Noguei...|     Livros|21482.94|\n",
      "|          3|        3|  Dalila Xavier Neto|     Música|15480.98|\n",
      "|          4|        4|           Davi Reis|     Jardim|11900.67|\n",
      "|          5|        5|     Warley Reis Jr.|     Música| 6097.59|\n",
      "|          6|        6|Srta. Maria Luiza...|Eletrônicos| 6501.28|\n",
      "|          7|        7|     Ricardo Batista|     Beleza| 5151.14|\n",
      "|          8|        8|Srta. Melissa Xavier|   Crianças| 2360.42|\n",
      "|          9|        9|         Kléber Reis|   Crianças| 7841.75|\n",
      "|         10|       10|         Silas Silva|    Sapatos|17572.83|\n",
      "|         11|       11|       Melissa Souza|     Roupas|25391.04|\n",
      "|         12|       12|        João Batista|      Jogos|  7907.0|\n",
      "|         13|       13|       Roberto Souza|     Jardim|21442.78|\n",
      "|         14|       14|      Helena Saraiva|      Jogos| 6897.37|\n",
      "|         15|       15|     Heloísa Martins|       Bebê| 5598.62|\n",
      "|         16|       16| Maria Cecília Costa|Ferramentas|22749.33|\n",
      "|         17|       17|    Isabela Carvalho| Automotivo|25050.54|\n",
      "|         18|       18|      Lorenzo Moraes|      Jóias|19628.12|\n",
      "|         19|       19|      Rafael Saraiva|      Jóias| 22944.5|\n",
      "|         20|       20|    Emanuelly Barros|     Livros|19004.18|\n",
      "+-----------+---------+--------------------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_unbalanced = (df_employees\n",
    "                 .join(df_salaries, on=\"salary_id\", how=\"inner\")\n",
    "                 .select(\"employee_id\", \"salary_id\", \"name\", \"department\", \"salary\")\n",
    "                 )\n",
    "\n",
    "df_unbalanced.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demonstrates a partition imbalance, where all data is in one partition out of the 5 defined by `spark.sql.shuffle.partitions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|partition_id|count|\n",
      "+------------+-----+\n",
      "|           0|10000|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df_unbalanced\n",
    " .withColumn(\"partition_id\", F.spark_partition_id())\n",
    " .groupBy(\"partition_id\")\n",
    " .count()\n",
    " .show()\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying salt hash join technique for improved partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new DataFrame based on `df_employees`, adding a `salt_id` column, which is generated by concatenating each employee's `salary_id` with a random value between 0 and 4.\n",
    "\n",
    "The DataFrame is then repartitioned by `salt_id`, resulting in a better partition balance due to the randomness introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "salt_col = F.concat_ws(\"_\", F.col(\"salary_id\"), (F.rand() * 5).cast(\"integer\"))\n",
    "\n",
    "df_employees_balanced = (df_employees\n",
    "                         .withColumn(\"salt_id\", salt_col)\n",
    "                         .repartition(\"salt_id\")\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a new DataFrame from `df_salaries`, also adding a `salt_id` column. This `salt_id` is formed by the Cartesian product of the `salary_id` column and the range 0 to 4.\n",
    "\n",
    "As before, the DataFrame is repartitioned using `salt_id` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_range = spark.range(0, SHUFFLE_PARTITIONS) # schema -> id: int\n",
    "\n",
    "df_salaries_balanced = (df_salaries\n",
    "                        .join(df_range, how=\"cross\")\n",
    "                        .withColumn(\"salt_id\", F.concat_ws(\"_\", F.col(\"salary_id\"), F.col(\"id\")))\n",
    "                        .repartition(\"salt_id\")\n",
    "                        .drop(F.col(\"id\"))\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining on `salt_id` and displaying employee and salary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------------------+------------+--------+-------+\n",
      "|employee_id|salary_id|               name|  department|  salary|salt_id|\n",
      "+-----------+---------+-------------------+------------+--------+-------+\n",
      "|          3|        3| Dalila Xavier Neto|      Música|15480.98|    3_1|\n",
      "|         13|       13|      Roberto Souza|      Jardim|21442.78|   13_0|\n",
      "|         15|       15|    Heloísa Martins|        Bebê| 5598.62|   15_3|\n",
      "|         17|       17|   Isabela Carvalho|  Automotivo|25050.54|   17_0|\n",
      "|         23|       23|  Marina Costa Neto|      Jardim| 17041.9|   23_4|\n",
      "|         24|       24|          Liz Silva|        Bebê|18754.61|   24_2|\n",
      "|         39|       39|    Carla Braga Jr.| Eletrônicos| 22664.0|   39_4|\n",
      "|         57|       57|    Henrique Moraes|       Saúde|17021.79|   57_0|\n",
      "|         60|       60|       Yasmin Souza|   Mercearia|20018.08|   60_4|\n",
      "|         61|       61|       Bruna Santos|   Mercearia|23166.84|   61_3|\n",
      "|         63|       63|       Heloísa Melo|        Casa| 1589.07|   63_0|\n",
      "|         66|       66|     Larissa Franco|    Esportes|25757.34|   66_2|\n",
      "|         69|       69|     Nicolas Franco|     Turismo| 9169.87|   69_1|\n",
      "|         74|       74|     Paula Oliveira|  Industrial| 7300.17|   74_4|\n",
      "|         75|       75|     Joana Nogueira|Computadores|19082.81|   75_0|\n",
      "|         84|       84|        Félix Silva|    Esportes| 6872.22|   84_0|\n",
      "|         86|       86|        César Souza|  Industrial|16875.68|   86_2|\n",
      "|         93|       93|Sr. Carlos Nogueira|    Crianças|26929.72|   93_2|\n",
      "|         99|       99|   Maria Clara Melo|     Turismo|22236.51|   99_2|\n",
      "|        104|      104|       Calebe Costa|       Jogos| 8440.95|  104_2|\n",
      "+-----------+---------+-------------------+------------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_balanced = (df_employees_balanced.alias(\"e\")\n",
    "               .join(df_salaries_balanced.alias(\"s\"), on=\"salt_id\", how=\"inner\")\n",
    "               .select(\"employee_id\", \"s.salary_id\", \"name\", \"department\", \"salary\", \"salt_id\")\n",
    "               )\n",
    "\n",
    "df_balanced.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demonstrates the balance across the 5 partitions defined by `spark.sql.shuffle.partitions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|partition_id|count|\n",
      "+------------+-----+\n",
      "|           0| 1961|\n",
      "|           1| 2007|\n",
      "|           2| 1997|\n",
      "|           3| 1998|\n",
      "|           4| 2037|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df_balanced\n",
    " .withColumn(\"partition_id\", F.spark_partition_id())\n",
    " .groupBy(\"partition_id\")\n",
    " .agg(F.count(\"*\").alias(\"count\"))\n",
    " .orderBy(\"partition_id\")\n",
    " .show()\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For small datasets, the performance difference is almost negligible, and may even worsen slightly. However, when dealing with Big Data in a clustered environment, techniques like this can lead to significant performance improvements and help prevent memory leaks due to more efficient shuffling.\n",
    "\n",
    "With the advent of [AQE](https://spark.apache.org/docs/latest/sql-performance-tuning.html#adaptive-query-execution), techniques like this are often unnecessary. However, it's always useful to be aware of alternative methods for solving such problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
