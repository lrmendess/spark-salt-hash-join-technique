{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SHUFFLE_PARTITIONS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/15 03:49:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/07/15 03:49:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"salaries\")\n",
    "         .config(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", SHUFFLE_PARTITIONS)\n",
    "         .getOrCreate()\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading employee and salary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employees = (spark\n",
    "                .read\n",
    "                .option(\"header\", True)\n",
    "                .option(\"delimiter\", \";\")\n",
    "                .option(\"inferSchema\", True)\n",
    "                .csv(\"inputs/employees_10000.csv\")\n",
    "                )\n",
    "\n",
    "df_salaries = (spark\n",
    "               .read\n",
    "               .option(\"header\", True)\n",
    "               .option(\"delimiter\", \";\")\n",
    "               .option(\"inferSchema\", True)\n",
    "               .csv(\"inputs/salaries_10000.csv\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration of poor distribution of data between partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining and demonstrating employee and salary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------------+-----------+--------+\n",
      "| id|salary_id|                name| department|  salary|\n",
      "+---+---------+--------------------+-----------+--------+\n",
      "|  1|        1|    Frederico Santos|  Mercearia|23708.05|\n",
      "|  2|        2|Alessandra Noguei...|     Livros|21482.94|\n",
      "|  3|        3|  Dalila Xavier Neto|     Música|15480.98|\n",
      "|  4|        4|           Davi Reis|     Jardim|11900.67|\n",
      "|  5|        5|     Warley Reis Jr.|     Música| 6097.59|\n",
      "|  6|        6|Srta. Maria Luiza...|Eletrônicos| 6501.28|\n",
      "|  7|        7|     Ricardo Batista|     Beleza| 5151.14|\n",
      "|  8|        8|Srta. Melissa Xavier|   Crianças| 2360.42|\n",
      "|  9|        9|         Kléber Reis|   Crianças| 7841.75|\n",
      "| 10|       10|         Silas Silva|    Sapatos|17572.83|\n",
      "| 11|       11|       Melissa Souza|     Roupas|25391.04|\n",
      "| 12|       12|        João Batista|      Jogos|  7907.0|\n",
      "| 13|       13|       Roberto Souza|     Jardim|21442.78|\n",
      "| 14|       14|      Helena Saraiva|      Jogos| 6897.37|\n",
      "| 15|       15|     Heloísa Martins|       Bebê| 5598.62|\n",
      "| 16|       16| Maria Cecília Costa|Ferramentas|22749.33|\n",
      "| 17|       17|    Isabela Carvalho| Automotivo|25050.54|\n",
      "| 18|       18|      Lorenzo Moraes|      Jóias|19628.12|\n",
      "| 19|       19|      Rafael Saraiva|      Jóias| 22944.5|\n",
      "| 20|       20|    Emanuelly Barros|     Livros|19004.18|\n",
      "+---+---------+--------------------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_unbalanced = (df_employees\n",
    "                 .join(df_salaries, on=\"salary_id\", how=\"inner\")\n",
    "                 .select(\"id\", \"salary_id\", \"name\", \"department\", \"salary\"))\n",
    "\n",
    "df_unbalanced.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration of partition imbalance, with all data in 1 partition of 5 defined in `spark.sql.shuffle.partitions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|partition_id|count|\n",
      "+------------+-----+\n",
      "|           0|10000|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df_unbalanced\n",
    " .withColumn(\"partition_id\", F.spark_partition_id())\n",
    " .groupBy(\"partition_id\")\n",
    " .count()\n",
    " .show()\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration of the effects of applying the salt hash join technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a new DataFrame based on df_employees, adding the \"salt_id\" column, composed of the concatenation of the employee's id with a random value between 0 and 4.\n",
    "\n",
    "Next, the DataFrame is repartitioned using the \"salt_id\" column, giving us better balance between partitions due to the randomness factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "salt_col = F.concat_ws(\"_\", F.col(\"salary_id\"), (F.rand() * 5).cast(\"integer\"))\n",
    "\n",
    "df_employees_balanced = (df_employees\n",
    "                         .withColumn(\"salt_id\", salt_col)\n",
    "                         .repartition(\"salt_id\")\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a new DataFrame based on df_salaries, also adding the `salt_id` column, now composed of the cartesian product of values ​​from 0 to 4 with the `salt_id` column of df_salaries, increasing the size of our DataFrame by 5 times.\n",
    "\n",
    "Just like before, the DataFrame is repartitioned using the `salt_id` too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_range = spark.range(0, SHUFFLE_PARTITIONS)\n",
    "\n",
    "df_salaries_balanced = (df_salaries.alias(\"s\")\n",
    "                        .join(df_range.alias(\"r\"), how=\"cross\")\n",
    "                        .withColumn(\"salt_id\", F.concat_ws(\"_\", F.col(\"salary_id\"), F.col(\"r.id\")))\n",
    "                        .repartition(\"salt_id\")\n",
    "                        .drop(F.col(\"r.id\"))\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining using `salt_id` column and demonstrating employee and salary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------------+------------+--------+-------+\n",
      "| id|salary_id|                name|  department|  salary|salt_id|\n",
      "+---+---------+--------------------+------------+--------+-------+\n",
      "|  5|        5|     Warley Reis Jr.|      Música| 6097.59|    5_0|\n",
      "|  6|        6|Srta. Maria Luiza...| Eletrônicos| 6501.28|    6_2|\n",
      "| 34|       34|   Fabiano Melo Neto|  Automotivo|17133.47|   34_4|\n",
      "| 52|       52|       Talita Barros|       Jóias|23644.01|   52_2|\n",
      "| 56|       56|        Lorena Braga|       Jóias| 9388.97|   56_2|\n",
      "| 58|       58|          Lara Souza|  Industrial|10079.65|   58_2|\n",
      "| 61|       61|        Bruna Santos|   Mercearia|23166.84|   61_3|\n",
      "| 63|       63|        Heloísa Melo|        Casa| 1589.07|   63_1|\n",
      "| 67|       67|     Yasmin Oliveira|       Jóias|18651.68|   67_1|\n",
      "| 75|       75|      Joana Nogueira|Computadores|19082.81|   75_3|\n",
      "| 83|       83|           Davi Melo|  Automotivo|20799.28|   83_4|\n",
      "| 84|       84|         Félix Silva|    Esportes| 6872.22|   84_0|\n",
      "| 85|       85|     Lorena Oliveira|      Roupas|16493.66|   85_3|\n",
      "| 86|       86|         César Souza|  Industrial|16875.68|   86_4|\n",
      "| 87|       87|      Rafaela Moraes|     Sapatos|16750.87|   87_1|\n",
      "| 90|       90|         Júlia Souza| Ferramentas|11552.88|   90_0|\n",
      "| 96|       96|      Daniel Batista|  Industrial|12600.56|   96_4|\n",
      "|101|      101|      Joaquim Santos|       Jogos|23106.29|  101_1|\n",
      "|102|      102|  Pietro Batista Jr.|      Roupas|26041.43|  102_1|\n",
      "|107|      107|   Enzo Franco Filho|    Crianças| 5719.16|  107_1|\n",
      "+---+---------+--------------------+------------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_balanced = (df_employees_balanced.alias(\"e\")\n",
    "               .join(df_salaries_balanced.alias(\"s\"), on=\"salt_id\", how=\"inner\")\n",
    "               .select(\"id\", \"s.salary_id\", \"name\", \"department\", \"salary\", \"salt_id\")\n",
    "               )\n",
    "\n",
    "df_balanced.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration of balance between the 5 partitions defined in `spark.sql.shuffle.partitions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|partition_id|count|\n",
      "+------------+-----+\n",
      "|           0| 1993|\n",
      "|           1| 1932|\n",
      "|           2| 1991|\n",
      "|           3| 1996|\n",
      "|           4| 2088|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df_balanced\n",
    " .withColumn(\"partition_id\", F.spark_partition_id())\n",
    " .groupBy(\"partition_id\")\n",
    " .agg(F.count(\"*\").alias(\"count\"))\n",
    " .orderBy(\"partition_id\")\n",
    " .show()\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For small masses of data, the difference in performance is almost imperceptible, probably even worse, but when we are talking about Big Data, especially in a clustered environment, this type of technique can bring huge performance gains and avoid memory leak problems, due to better use of shuffling.\n",
    "\n",
    "With the emergence of [AQE](https://spark.apache.org/docs/latest/sql-performance-tuning.html#adaptive-query-execution), strategies like this end up not being necessary most of the time, but it is always good to keep in mind alternative ways to solve this type of problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
